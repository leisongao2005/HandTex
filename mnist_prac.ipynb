{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "DEVICE = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating One layer NN\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, device=DEVICE),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8, device=DEVICE),\n",
    "            nn.Conv2d(8, 64, kernel_size=5, device=DEVICE),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64, device=DEVICE),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lin_layers = nn.Sequential(\n",
    "            nn.Linear(1024, 512, device=DEVICE),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 128, device=DEVICE),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 10, device=DEVICE),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.conv_layers(input)\n",
    "        output = self.lin_layers(output.view(-1, 1024))\n",
    "        logits = F.softmax(output, dim=1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SimpleNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code for train/validation and test sets\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST('../', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = MNIST('../', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, [50000, 10000], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 100 | Loss 1.53892\n",
      "Epoch 0 | Batch 200 | Loss 1.58925\n",
      "Epoch 0 | Batch 300 | Loss 1.48736\n",
      "Epoch 0 | Batch 400 | Loss 1.51367\n",
      "Epoch 0 | Batch 500 | Loss 1.54900\n",
      "Epoch 0 | Batch 600 | Loss 1.48520\n",
      "Epoch 0 | Batch 700 | Loss 1.47099\n",
      "Epoch 0 | Training Loss 1.54213 | Validation Loss 1.48711\n",
      "Epoch 1 | Batch 100 | Loss 1.47840\n",
      "Epoch 1 | Batch 200 | Loss 1.48898\n",
      "Epoch 1 | Batch 300 | Loss 1.47694\n",
      "Epoch 1 | Batch 400 | Loss 1.47737\n",
      "Epoch 1 | Batch 500 | Loss 1.48557\n",
      "Epoch 1 | Batch 600 | Loss 1.47196\n",
      "Epoch 1 | Batch 700 | Loss 1.47936\n",
      "Epoch 1 | Training Loss 1.48939 | Validation Loss 1.48331\n",
      "Epoch 2 | Batch 100 | Loss 1.47494\n",
      "Epoch 2 | Batch 200 | Loss 1.51436\n",
      "Epoch 2 | Batch 300 | Loss 1.46601\n",
      "Epoch 2 | Batch 400 | Loss 1.47128\n",
      "Epoch 2 | Batch 500 | Loss 1.49176\n",
      "Epoch 2 | Batch 600 | Loss 1.46372\n",
      "Epoch 2 | Batch 700 | Loss 1.49648\n",
      "Epoch 2 | Training Loss 1.48568 | Validation Loss 1.47912\n",
      "Epoch 3 | Batch 100 | Loss 1.49561\n",
      "Epoch 3 | Batch 200 | Loss 1.47474\n",
      "Epoch 3 | Batch 300 | Loss 1.47738\n",
      "Epoch 3 | Batch 400 | Loss 1.46834\n",
      "Epoch 3 | Batch 500 | Loss 1.46117\n",
      "Epoch 3 | Batch 600 | Loss 1.49698\n",
      "Epoch 3 | Batch 700 | Loss 1.46338\n",
      "Epoch 3 | Training Loss 1.48291 | Validation Loss 1.47586\n",
      "Epoch 4 | Batch 100 | Loss 1.47678\n",
      "Epoch 4 | Batch 200 | Loss 1.47852\n",
      "Epoch 4 | Batch 300 | Loss 1.47589\n",
      "Epoch 4 | Batch 400 | Loss 1.46115\n",
      "Epoch 4 | Batch 500 | Loss 1.47678\n",
      "Epoch 4 | Batch 600 | Loss 1.46202\n",
      "Epoch 4 | Batch 700 | Loss 1.49585\n",
      "Epoch 4 | Training Loss 1.47939 | Validation Loss 1.48023\n",
      "Epoch 5 | Batch 100 | Loss 1.46394\n",
      "Epoch 5 | Batch 200 | Loss 1.47672\n",
      "Epoch 5 | Batch 300 | Loss 1.47676\n",
      "Epoch 5 | Batch 400 | Loss 1.47667\n",
      "Epoch 5 | Batch 500 | Loss 1.47761\n",
      "Epoch 5 | Batch 600 | Loss 1.49308\n",
      "Epoch 5 | Batch 700 | Loss 1.46264\n",
      "Epoch 5 | Training Loss 1.48013 | Validation Loss 1.47588\n",
      "Epoch 6 | Batch 100 | Loss 1.47610\n",
      "Epoch 6 | Batch 200 | Loss 1.46117\n",
      "Epoch 6 | Batch 300 | Loss 1.47678\n",
      "Epoch 6 | Batch 400 | Loss 1.50788\n",
      "Epoch 6 | Batch 500 | Loss 1.46120\n",
      "Epoch 6 | Batch 600 | Loss 1.46115\n",
      "Epoch 6 | Batch 700 | Loss 1.47643\n",
      "Epoch 6 | Training Loss 1.48125 | Validation Loss 1.47754\n",
      "Epoch 7 | Batch 100 | Loss 1.46115\n",
      "Epoch 7 | Batch 200 | Loss 1.46115\n",
      "Epoch 7 | Batch 300 | Loss 1.47675\n",
      "Epoch 7 | Batch 400 | Loss 1.47678\n",
      "Epoch 7 | Batch 500 | Loss 1.46133\n",
      "Epoch 7 | Batch 600 | Loss 1.49234\n",
      "Epoch 7 | Batch 700 | Loss 1.47678\n",
      "Epoch 7 | Training Loss 1.47836 | Validation Loss 1.47939\n",
      "Epoch 8 | Batch 100 | Loss 1.50462\n",
      "Epoch 8 | Batch 200 | Loss 1.47678\n",
      "Epoch 8 | Batch 300 | Loss 1.47811\n",
      "Epoch 8 | Batch 400 | Loss 1.53994\n",
      "Epoch 8 | Batch 500 | Loss 1.49240\n",
      "Epoch 8 | Batch 600 | Loss 1.46115\n",
      "Epoch 8 | Batch 700 | Loss 1.46315\n",
      "Epoch 8 | Training Loss 1.47699 | Validation Loss 1.47459\n",
      "Epoch 9 | Batch 100 | Loss 1.46115\n",
      "Epoch 9 | Batch 200 | Loss 1.46367\n",
      "Epoch 9 | Batch 300 | Loss 1.47781\n",
      "Epoch 9 | Batch 400 | Loss 1.47660\n",
      "Epoch 9 | Batch 500 | Loss 1.50648\n",
      "Epoch 9 | Batch 600 | Loss 1.49094\n",
      "Epoch 9 | Batch 700 | Loss 1.49701\n",
      "Epoch 9 | Training Loss 1.47672 | Validation Loss 1.47364\n",
      "Epoch 10 | Batch 100 | Loss 1.49222\n",
      "Epoch 10 | Batch 200 | Loss 1.49240\n",
      "Epoch 10 | Batch 300 | Loss 1.50627\n",
      "Epoch 10 | Batch 400 | Loss 1.48370\n",
      "Epoch 10 | Batch 500 | Loss 1.49240\n",
      "Epoch 10 | Batch 600 | Loss 1.46115\n",
      "Epoch 10 | Batch 700 | Loss 1.47678\n",
      "Epoch 10 | Training Loss 1.47793 | Validation Loss 1.47742\n",
      "Epoch 11 | Batch 100 | Loss 1.47684\n",
      "Epoch 11 | Batch 200 | Loss 1.49227\n",
      "Epoch 11 | Batch 300 | Loss 1.49240\n",
      "Epoch 11 | Batch 400 | Loss 1.46128\n",
      "Epoch 11 | Batch 500 | Loss 1.49574\n",
      "Epoch 11 | Batch 600 | Loss 1.47678\n",
      "Epoch 11 | Batch 700 | Loss 1.47708\n",
      "Epoch 11 | Training Loss 1.47848 | Validation Loss 1.47700\n",
      "Epoch 12 | Batch 100 | Loss 1.49469\n",
      "Epoch 12 | Batch 200 | Loss 1.47649\n",
      "Epoch 12 | Batch 300 | Loss 1.49240\n",
      "Epoch 12 | Batch 400 | Loss 1.49002\n",
      "Epoch 12 | Batch 500 | Loss 1.48414\n",
      "Epoch 12 | Batch 600 | Loss 1.47678\n",
      "Epoch 12 | Batch 700 | Loss 1.46115\n",
      "Epoch 12 | Training Loss 1.47677 | Validation Loss 1.47481\n",
      "Epoch 13 | Batch 100 | Loss 1.46115\n",
      "Epoch 13 | Batch 200 | Loss 1.46126\n",
      "Epoch 13 | Batch 300 | Loss 1.47659\n",
      "Epoch 13 | Batch 400 | Loss 1.47587\n",
      "Epoch 13 | Batch 500 | Loss 1.47678\n",
      "Epoch 13 | Batch 600 | Loss 1.47651\n",
      "Epoch 13 | Batch 700 | Loss 1.46115\n",
      "Epoch 13 | Training Loss 1.47690 | Validation Loss 1.47430\n",
      "Epoch 14 | Batch 100 | Loss 1.47678\n",
      "Epoch 14 | Batch 200 | Loss 1.46162\n",
      "Epoch 14 | Batch 300 | Loss 1.46116\n",
      "Epoch 14 | Batch 400 | Loss 1.46195\n",
      "Epoch 14 | Batch 500 | Loss 1.47678\n",
      "Epoch 14 | Batch 600 | Loss 1.47768\n",
      "Epoch 14 | Batch 700 | Loss 1.47695\n",
      "Epoch 14 | Training Loss 1.47650 | Validation Loss 1.47403\n",
      "Epoch 15 | Batch 100 | Loss 1.49239\n",
      "Epoch 15 | Batch 200 | Loss 1.47678\n",
      "Epoch 15 | Batch 300 | Loss 1.48578\n",
      "Epoch 15 | Batch 400 | Loss 1.52382\n",
      "Epoch 15 | Batch 500 | Loss 1.46115\n",
      "Epoch 15 | Batch 600 | Loss 1.49240\n",
      "Epoch 15 | Batch 700 | Loss 1.47686\n",
      "Epoch 15 | Training Loss 1.47837 | Validation Loss 1.47372\n",
      "Epoch 16 | Batch 100 | Loss 1.46115\n",
      "Epoch 16 | Batch 200 | Loss 1.46115\n",
      "Epoch 16 | Batch 300 | Loss 1.47678\n",
      "Epoch 16 | Batch 400 | Loss 1.46115\n",
      "Epoch 16 | Batch 500 | Loss 1.46115\n",
      "Epoch 16 | Batch 600 | Loss 1.49392\n",
      "Epoch 16 | Batch 700 | Loss 1.46115\n",
      "Epoch 16 | Training Loss 1.47693 | Validation Loss 1.47445\n",
      "Epoch 17 | Batch 100 | Loss 1.47380\n",
      "Epoch 17 | Batch 200 | Loss 1.46115\n",
      "Epoch 17 | Batch 300 | Loss 1.50803\n",
      "Epoch 17 | Batch 400 | Loss 1.46115\n",
      "Epoch 17 | Batch 500 | Loss 1.46115\n",
      "Epoch 17 | Batch 600 | Loss 1.47678\n",
      "Epoch 17 | Batch 700 | Loss 1.47695\n",
      "Epoch 17 | Training Loss 1.47530 | Validation Loss 1.47414\n",
      "Epoch 18 | Batch 100 | Loss 1.46115\n",
      "Epoch 18 | Batch 200 | Loss 1.46116\n",
      "Epoch 18 | Batch 300 | Loss 1.49240\n",
      "Epoch 18 | Batch 400 | Loss 1.52412\n",
      "Epoch 18 | Batch 500 | Loss 1.47678\n",
      "Epoch 18 | Batch 600 | Loss 1.49084\n",
      "Epoch 18 | Batch 700 | Loss 1.46115\n",
      "Epoch 18 | Training Loss 1.47491 | Validation Loss 1.47290\n",
      "Epoch 19 | Batch 100 | Loss 1.46115\n",
      "Epoch 19 | Batch 200 | Loss 1.46115\n",
      "Epoch 19 | Batch 300 | Loss 1.47633\n",
      "Epoch 19 | Batch 400 | Loss 1.46115\n",
      "Epoch 19 | Batch 500 | Loss 1.46115\n",
      "Epoch 19 | Batch 600 | Loss 1.46115\n",
      "Epoch 19 | Batch 700 | Loss 1.47706\n",
      "Epoch 19 | Training Loss 1.47516 | Validation Loss 1.47573\n",
      "Epoch 20 | Batch 100 | Loss 1.46115\n",
      "Epoch 20 | Batch 200 | Loss 1.49240\n",
      "Epoch 20 | Batch 300 | Loss 1.46116\n",
      "Epoch 20 | Batch 400 | Loss 1.46115\n",
      "Epoch 20 | Batch 500 | Loss 1.46115\n",
      "Epoch 20 | Batch 600 | Loss 1.46115\n",
      "Epoch 20 | Batch 700 | Loss 1.46115\n",
      "Epoch 20 | Training Loss 1.47444 | Validation Loss 1.47415\n",
      "Epoch 21 | Batch 100 | Loss 1.49200\n",
      "Epoch 21 | Batch 200 | Loss 1.49280\n",
      "Epoch 21 | Batch 300 | Loss 1.46714\n",
      "Epoch 21 | Batch 400 | Loss 1.46115\n",
      "Epoch 21 | Batch 500 | Loss 1.47678\n",
      "Epoch 21 | Batch 600 | Loss 1.48187\n",
      "Epoch 21 | Batch 700 | Loss 1.46115\n",
      "Epoch 21 | Training Loss 1.47631 | Validation Loss 1.47166\n",
      "Epoch 22 | Batch 100 | Loss 1.47676\n",
      "Epoch 22 | Batch 200 | Loss 1.47678\n",
      "Epoch 22 | Batch 300 | Loss 1.47678\n",
      "Epoch 22 | Batch 400 | Loss 1.46115\n",
      "Epoch 22 | Batch 500 | Loss 1.46115\n",
      "Epoch 22 | Batch 600 | Loss 1.47678\n",
      "Epoch 22 | Batch 700 | Loss 1.47678\n",
      "Epoch 22 | Training Loss 1.47562 | Validation Loss 1.47558\n",
      "Epoch 23 | Batch 100 | Loss 1.49240\n",
      "Epoch 23 | Batch 200 | Loss 1.47678\n",
      "Epoch 23 | Batch 300 | Loss 1.47678\n",
      "Epoch 23 | Batch 400 | Loss 1.49240\n",
      "Epoch 23 | Batch 500 | Loss 1.47678\n",
      "Epoch 23 | Batch 600 | Loss 1.49240\n",
      "Epoch 23 | Batch 700 | Loss 1.48924\n",
      "Epoch 23 | Training Loss 1.47581 | Validation Loss 1.47475\n",
      "Epoch 24 | Batch 100 | Loss 1.46115\n",
      "Epoch 24 | Batch 200 | Loss 1.48877\n",
      "Epoch 24 | Batch 300 | Loss 1.46115\n",
      "Epoch 24 | Batch 400 | Loss 1.47678\n",
      "Epoch 24 | Batch 500 | Loss 1.47678\n",
      "Epoch 24 | Batch 600 | Loss 1.47678\n",
      "Epoch 24 | Batch 700 | Loss 1.46115\n",
      "Epoch 24 | Training Loss 1.47557 | Validation Loss 1.47309\n",
      "Epoch 25 | Batch 100 | Loss 1.46115\n",
      "Epoch 25 | Batch 200 | Loss 1.46115\n",
      "Epoch 25 | Batch 300 | Loss 1.46115\n",
      "Epoch 25 | Batch 400 | Loss 1.49894\n",
      "Epoch 25 | Batch 500 | Loss 1.46115\n",
      "Epoch 25 | Batch 600 | Loss 1.47678\n",
      "Epoch 25 | Batch 700 | Loss 1.46115\n",
      "Epoch 25 | Training Loss 1.47447 | Validation Loss 1.47203\n",
      "Epoch 26 | Batch 100 | Loss 1.46115\n",
      "Epoch 26 | Batch 200 | Loss 1.47619\n",
      "Epoch 26 | Batch 300 | Loss 1.46115\n",
      "Epoch 26 | Batch 400 | Loss 1.49240\n",
      "Epoch 26 | Batch 500 | Loss 1.46115\n",
      "Epoch 26 | Batch 600 | Loss 1.49240\n",
      "Epoch 26 | Batch 700 | Loss 1.49902\n",
      "Epoch 26 | Training Loss 1.47476 | Validation Loss 1.47195\n",
      "Epoch 27 | Batch 100 | Loss 1.47628\n",
      "Epoch 27 | Batch 200 | Loss 1.49240\n",
      "Epoch 27 | Batch 300 | Loss 1.49240\n",
      "Epoch 27 | Batch 400 | Loss 1.47678\n",
      "Epoch 27 | Batch 500 | Loss 1.46119\n",
      "Epoch 27 | Batch 600 | Loss 1.46116\n",
      "Epoch 27 | Batch 700 | Loss 1.48161\n",
      "Epoch 27 | Training Loss 1.47348 | Validation Loss 1.47334\n",
      "Epoch 28 | Batch 100 | Loss 1.47230\n",
      "Epoch 28 | Batch 200 | Loss 1.46726\n",
      "Epoch 28 | Batch 300 | Loss 1.49240\n",
      "Epoch 28 | Batch 400 | Loss 1.46115\n",
      "Epoch 28 | Batch 500 | Loss 1.47683\n",
      "Epoch 28 | Batch 600 | Loss 1.46115\n",
      "Epoch 28 | Batch 700 | Loss 1.46115\n",
      "Epoch 28 | Training Loss 1.47515 | Validation Loss 1.47378\n",
      "Epoch 29 | Batch 100 | Loss 1.46115\n",
      "Epoch 29 | Batch 200 | Loss 1.49240\n",
      "Epoch 29 | Batch 300 | Loss 1.47678\n",
      "Epoch 29 | Batch 400 | Loss 1.47678\n",
      "Epoch 29 | Batch 500 | Loss 1.47677\n",
      "Epoch 29 | Batch 600 | Loss 1.46115\n",
      "Epoch 29 | Batch 700 | Loss 1.49240\n",
      "Epoch 29 | Training Loss 1.47236 | Validation Loss 1.47145\n",
      "Epoch 30 | Batch 100 | Loss 1.46121\n",
      "Epoch 30 | Batch 200 | Loss 1.46115\n",
      "Epoch 30 | Batch 300 | Loss 1.49240\n",
      "Epoch 30 | Batch 400 | Loss 1.46115\n",
      "Epoch 30 | Batch 500 | Loss 1.46115\n",
      "Epoch 30 | Batch 600 | Loss 1.46115\n",
      "Epoch 30 | Batch 700 | Loss 1.49186\n",
      "Epoch 30 | Training Loss 1.47386 | Validation Loss 1.47304\n",
      "Epoch 31 | Batch 100 | Loss 1.46115\n",
      "Epoch 31 | Batch 200 | Loss 1.47678\n",
      "Epoch 31 | Batch 300 | Loss 1.46115\n",
      "Epoch 31 | Batch 400 | Loss 1.49231\n",
      "Epoch 31 | Batch 500 | Loss 1.49240\n",
      "Epoch 31 | Batch 600 | Loss 1.46115\n",
      "Epoch 31 | Batch 700 | Loss 1.46115\n",
      "Epoch 31 | Training Loss 1.47300 | Validation Loss 1.47051\n",
      "Epoch 32 | Batch 100 | Loss 1.47678\n",
      "Epoch 32 | Batch 200 | Loss 1.47678\n",
      "Epoch 32 | Batch 300 | Loss 1.46115\n",
      "Epoch 32 | Batch 400 | Loss 1.46115\n",
      "Epoch 32 | Batch 500 | Loss 1.49240\n",
      "Epoch 32 | Batch 600 | Loss 1.46115\n",
      "Epoch 32 | Batch 700 | Loss 1.46115\n",
      "Epoch 32 | Training Loss 1.47289 | Validation Loss 1.47101\n",
      "Epoch 33 | Batch 100 | Loss 1.49240\n",
      "Epoch 33 | Batch 200 | Loss 1.46115\n",
      "Epoch 33 | Batch 300 | Loss 1.47678\n",
      "Epoch 33 | Batch 400 | Loss 1.46115\n",
      "Epoch 33 | Batch 500 | Loss 1.46116\n",
      "Epoch 33 | Batch 600 | Loss 1.46115\n",
      "Epoch 33 | Batch 700 | Loss 1.49252\n",
      "Epoch 33 | Training Loss 1.47290 | Validation Loss 1.47329\n",
      "Epoch 34 | Batch 100 | Loss 1.49236\n",
      "Epoch 34 | Batch 200 | Loss 1.47678\n",
      "Epoch 34 | Batch 300 | Loss 1.46115\n",
      "Epoch 34 | Batch 400 | Loss 1.46115\n",
      "Epoch 34 | Batch 500 | Loss 1.47678\n",
      "Epoch 34 | Batch 600 | Loss 1.46115\n",
      "Epoch 34 | Batch 700 | Loss 1.46115\n",
      "Epoch 34 | Training Loss 1.47200 | Validation Loss 1.47111\n",
      "Epoch 35 | Batch 100 | Loss 1.47684\n",
      "Epoch 35 | Batch 200 | Loss 1.47540\n",
      "Epoch 35 | Batch 300 | Loss 1.47391\n",
      "Epoch 35 | Batch 400 | Loss 1.46115\n",
      "Epoch 35 | Batch 500 | Loss 1.48940\n",
      "Epoch 35 | Batch 600 | Loss 1.47677\n",
      "Epoch 35 | Batch 700 | Loss 1.46115\n",
      "Epoch 35 | Training Loss 1.47291 | Validation Loss 1.47246\n",
      "Epoch 36 | Batch 100 | Loss 1.47678\n",
      "Epoch 36 | Batch 200 | Loss 1.47678\n",
      "Epoch 36 | Batch 300 | Loss 1.47678\n",
      "Epoch 36 | Batch 400 | Loss 1.47621\n",
      "Epoch 36 | Batch 500 | Loss 1.47665\n",
      "Epoch 36 | Batch 600 | Loss 1.49233\n",
      "Epoch 36 | Batch 700 | Loss 1.47678\n",
      "Epoch 36 | Training Loss 1.47366 | Validation Loss 1.47621\n",
      "Epoch 37 | Batch 100 | Loss 1.47660\n",
      "Epoch 37 | Batch 200 | Loss 1.46375\n",
      "Epoch 37 | Batch 300 | Loss 1.51678\n",
      "Epoch 37 | Batch 400 | Loss 1.47998\n",
      "Epoch 37 | Batch 500 | Loss 1.47677\n",
      "Epoch 37 | Batch 600 | Loss 1.47678\n",
      "Epoch 37 | Batch 700 | Loss 1.46115\n",
      "Epoch 37 | Training Loss 1.47397 | Validation Loss 1.47148\n",
      "Epoch 38 | Batch 100 | Loss 1.46115\n",
      "Epoch 38 | Batch 200 | Loss 1.47678\n",
      "Epoch 38 | Batch 300 | Loss 1.47617\n",
      "Epoch 38 | Batch 400 | Loss 1.46115\n",
      "Epoch 38 | Batch 500 | Loss 1.46115\n",
      "Epoch 38 | Batch 600 | Loss 1.47677\n",
      "Epoch 38 | Batch 700 | Loss 1.47677\n",
      "Epoch 38 | Training Loss 1.47238 | Validation Loss 1.47345\n",
      "Epoch 39 | Batch 100 | Loss 1.49240\n",
      "Epoch 39 | Batch 200 | Loss 1.46115\n",
      "Epoch 39 | Batch 300 | Loss 1.46115\n",
      "Epoch 39 | Batch 400 | Loss 1.47681\n",
      "Epoch 39 | Batch 500 | Loss 1.47678\n",
      "Epoch 39 | Batch 600 | Loss 1.52323\n",
      "Epoch 39 | Batch 700 | Loss 1.47678\n",
      "Epoch 39 | Training Loss 1.47239 | Validation Loss 1.47155\n",
      "Epoch 40 | Batch 100 | Loss 1.49240\n",
      "Epoch 40 | Batch 200 | Loss 1.46115\n",
      "Epoch 40 | Batch 300 | Loss 1.46115\n",
      "Epoch 40 | Batch 400 | Loss 1.46115\n",
      "Epoch 40 | Batch 500 | Loss 1.46115\n",
      "Epoch 40 | Batch 600 | Loss 1.46115\n",
      "Epoch 40 | Batch 700 | Loss 1.47388\n",
      "Epoch 40 | Training Loss 1.47385 | Validation Loss 1.47240\n",
      "Epoch 41 | Batch 100 | Loss 1.46115\n",
      "Epoch 41 | Batch 200 | Loss 1.47678\n",
      "Epoch 41 | Batch 300 | Loss 1.49244\n",
      "Epoch 41 | Batch 400 | Loss 1.46115\n",
      "Epoch 41 | Batch 500 | Loss 1.46621\n",
      "Epoch 41 | Batch 600 | Loss 1.46115\n",
      "Epoch 41 | Batch 700 | Loss 1.47678\n",
      "Epoch 41 | Training Loss 1.47293 | Validation Loss 1.47240\n",
      "Epoch 42 | Batch 100 | Loss 1.46121\n",
      "Epoch 42 | Batch 200 | Loss 1.47678\n",
      "Epoch 42 | Batch 300 | Loss 1.46115\n",
      "Epoch 42 | Batch 400 | Loss 1.46115\n",
      "Epoch 42 | Batch 500 | Loss 1.47678\n",
      "Epoch 42 | Batch 600 | Loss 1.47678\n",
      "Epoch 42 | Batch 700 | Loss 1.47627\n",
      "Epoch 42 | Training Loss 1.47249 | Validation Loss 1.47310\n",
      "Epoch 43 | Batch 100 | Loss 1.46115\n",
      "Epoch 43 | Batch 200 | Loss 1.47272\n",
      "Epoch 43 | Batch 300 | Loss 1.48244\n",
      "Epoch 43 | Batch 400 | Loss 1.46115\n",
      "Epoch 43 | Batch 500 | Loss 1.49240\n",
      "Epoch 43 | Batch 600 | Loss 1.48485\n",
      "Epoch 43 | Batch 700 | Loss 1.46115\n",
      "Epoch 43 | Training Loss 1.47221 | Validation Loss 1.47136\n",
      "Epoch 44 | Batch 100 | Loss 1.47678\n",
      "Epoch 44 | Batch 200 | Loss 1.47678\n",
      "Epoch 44 | Batch 300 | Loss 1.46115\n",
      "Epoch 44 | Batch 400 | Loss 1.50801\n",
      "Epoch 44 | Batch 500 | Loss 1.46115\n",
      "Epoch 44 | Batch 600 | Loss 1.46115\n",
      "Epoch 44 | Batch 700 | Loss 1.46115\n",
      "Epoch 44 | Training Loss 1.47155 | Validation Loss 1.47239\n",
      "Epoch 45 | Batch 100 | Loss 1.49194\n",
      "Epoch 45 | Batch 200 | Loss 1.49240\n",
      "Epoch 45 | Batch 300 | Loss 1.46115\n",
      "Epoch 45 | Batch 400 | Loss 1.52111\n",
      "Epoch 45 | Batch 500 | Loss 1.47681\n",
      "Epoch 45 | Batch 600 | Loss 1.46115\n",
      "Epoch 45 | Batch 700 | Loss 1.46115\n",
      "Epoch 45 | Training Loss 1.47244 | Validation Loss 1.47062\n",
      "Epoch 46 | Batch 100 | Loss 1.47678\n",
      "Epoch 46 | Batch 200 | Loss 1.47678\n",
      "Epoch 46 | Batch 300 | Loss 1.47672\n",
      "Epoch 46 | Batch 400 | Loss 1.49780\n",
      "Epoch 46 | Batch 500 | Loss 1.46115\n",
      "Epoch 46 | Batch 600 | Loss 1.46115\n",
      "Epoch 46 | Batch 700 | Loss 1.49240\n",
      "Epoch 46 | Training Loss 1.47078 | Validation Loss 1.46935\n",
      "Epoch 47 | Batch 100 | Loss 1.47678\n",
      "Epoch 47 | Batch 200 | Loss 1.46115\n",
      "Epoch 47 | Batch 300 | Loss 1.46115\n",
      "Epoch 47 | Batch 400 | Loss 1.46115\n",
      "Epoch 47 | Batch 500 | Loss 1.47678\n",
      "Epoch 47 | Batch 600 | Loss 1.46115\n",
      "Epoch 47 | Batch 700 | Loss 1.49240\n",
      "Epoch 47 | Training Loss 1.47125 | Validation Loss 1.47060\n",
      "Epoch 48 | Batch 100 | Loss 1.46115\n",
      "Epoch 48 | Batch 200 | Loss 1.46206\n",
      "Epoch 48 | Batch 300 | Loss 1.47675\n",
      "Epoch 48 | Batch 400 | Loss 1.49240\n",
      "Epoch 48 | Batch 500 | Loss 1.46116\n",
      "Epoch 48 | Batch 600 | Loss 1.47678\n",
      "Epoch 48 | Batch 700 | Loss 1.47678\n",
      "Epoch 48 | Training Loss 1.47196 | Validation Loss 1.47221\n",
      "Epoch 49 | Batch 100 | Loss 1.46115\n",
      "Epoch 49 | Batch 200 | Loss 1.46115\n",
      "Epoch 49 | Batch 300 | Loss 1.47678\n",
      "Epoch 49 | Batch 400 | Loss 1.46115\n",
      "Epoch 49 | Batch 500 | Loss 1.47678\n",
      "Epoch 49 | Batch 600 | Loss 1.46115\n",
      "Epoch 49 | Batch 700 | Loss 1.46115\n",
      "Epoch 49 | Training Loss 1.47153 | Validation Loss 1.47167\n",
      "Epoch 50 | Batch 100 | Loss 1.47677\n",
      "Epoch 50 | Batch 200 | Loss 1.50776\n",
      "Epoch 50 | Batch 300 | Loss 1.46115\n",
      "Epoch 50 | Batch 400 | Loss 1.47678\n",
      "Epoch 50 | Batch 500 | Loss 1.49240\n",
      "Epoch 50 | Batch 600 | Loss 1.47678\n",
      "Epoch 50 | Batch 700 | Loss 1.49240\n",
      "Epoch 50 | Training Loss 1.47161 | Validation Loss 1.46924\n",
      "Epoch 51 | Batch 100 | Loss 1.46115\n",
      "Epoch 51 | Batch 200 | Loss 1.46115\n",
      "Epoch 51 | Batch 300 | Loss 1.46275\n",
      "Epoch 51 | Batch 400 | Loss 1.46115\n",
      "Epoch 51 | Batch 500 | Loss 1.48410\n",
      "Epoch 51 | Batch 600 | Loss 1.46115\n",
      "Epoch 51 | Batch 700 | Loss 1.47548\n",
      "Epoch 51 | Training Loss 1.47125 | Validation Loss 1.47106\n",
      "Epoch 52 | Batch 100 | Loss 1.47237\n",
      "Epoch 52 | Batch 200 | Loss 1.47674\n",
      "Epoch 52 | Batch 300 | Loss 1.46115\n",
      "Epoch 52 | Batch 400 | Loss 1.46115\n",
      "Epoch 52 | Batch 500 | Loss 1.46115\n",
      "Epoch 52 | Batch 600 | Loss 1.46122\n",
      "Epoch 52 | Batch 700 | Loss 1.46115\n",
      "Epoch 52 | Training Loss 1.47137 | Validation Loss 1.47070\n",
      "Epoch 53 | Batch 100 | Loss 1.46115\n",
      "Epoch 53 | Batch 200 | Loss 1.49240\n",
      "Epoch 53 | Batch 300 | Loss 1.46115\n",
      "Epoch 53 | Batch 400 | Loss 1.46115\n",
      "Epoch 53 | Batch 500 | Loss 1.47678\n",
      "Epoch 53 | Batch 600 | Loss 1.46115\n",
      "Epoch 53 | Batch 700 | Loss 1.49100\n",
      "Epoch 53 | Training Loss 1.47117 | Validation Loss 1.47295\n",
      "Epoch 54 | Batch 100 | Loss 1.47678\n",
      "Epoch 54 | Batch 200 | Loss 1.49245\n",
      "Epoch 54 | Batch 300 | Loss 1.47678\n",
      "Epoch 54 | Batch 400 | Loss 1.47656\n",
      "Epoch 54 | Batch 500 | Loss 1.46115\n",
      "Epoch 54 | Batch 600 | Loss 1.46115\n",
      "Epoch 54 | Batch 700 | Loss 1.46116\n",
      "Epoch 54 | Training Loss 1.47171 | Validation Loss 1.47179\n",
      "Epoch 55 | Batch 100 | Loss 1.46115\n",
      "Epoch 55 | Batch 200 | Loss 1.46115\n",
      "Epoch 55 | Batch 300 | Loss 1.46115\n",
      "Epoch 55 | Batch 400 | Loss 1.46115\n",
      "Epoch 55 | Batch 500 | Loss 1.47678\n",
      "Epoch 55 | Batch 600 | Loss 1.46154\n",
      "Epoch 55 | Batch 700 | Loss 1.47793\n",
      "Epoch 55 | Training Loss 1.47274 | Validation Loss 1.47166\n",
      "Epoch 56 | Batch 100 | Loss 1.46115\n",
      "Epoch 56 | Batch 200 | Loss 1.49612\n",
      "Epoch 56 | Batch 300 | Loss 1.47678\n",
      "Epoch 56 | Batch 400 | Loss 1.46115\n",
      "Epoch 56 | Batch 500 | Loss 1.47677\n",
      "Epoch 56 | Batch 600 | Loss 1.46188\n",
      "Epoch 56 | Batch 700 | Loss 1.47748\n",
      "Epoch 56 | Training Loss 1.47085 | Validation Loss 1.46902\n",
      "Epoch 57 | Batch 100 | Loss 1.46115\n",
      "Epoch 57 | Batch 200 | Loss 1.46117\n",
      "Epoch 57 | Batch 300 | Loss 1.47678\n",
      "Epoch 57 | Batch 400 | Loss 1.49220\n",
      "Epoch 57 | Batch 500 | Loss 1.49240\n",
      "Epoch 57 | Batch 600 | Loss 1.46115\n",
      "Epoch 57 | Batch 700 | Loss 1.46115\n",
      "Epoch 57 | Training Loss 1.47011 | Validation Loss 1.47112\n",
      "Epoch 58 | Batch 100 | Loss 1.46115\n",
      "Epoch 58 | Batch 200 | Loss 1.46115\n",
      "Epoch 58 | Batch 300 | Loss 1.49240\n",
      "Epoch 58 | Batch 400 | Loss 1.47678\n",
      "Epoch 58 | Batch 500 | Loss 1.47933\n",
      "Epoch 58 | Batch 600 | Loss 1.49238\n",
      "Epoch 58 | Batch 700 | Loss 1.47259\n",
      "Epoch 58 | Training Loss 1.47120 | Validation Loss 1.47189\n",
      "Epoch 59 | Batch 100 | Loss 1.47678\n",
      "Epoch 59 | Batch 200 | Loss 1.46115\n",
      "Epoch 59 | Batch 300 | Loss 1.47678\n",
      "Epoch 59 | Batch 400 | Loss 1.47678\n",
      "Epoch 59 | Batch 500 | Loss 1.46115\n",
      "Epoch 59 | Batch 600 | Loss 1.46117\n",
      "Epoch 59 | Batch 700 | Loss 1.46115\n",
      "Epoch 59 | Training Loss 1.47099 | Validation Loss 1.47374\n",
      "Epoch 60 | Batch 100 | Loss 1.49240\n",
      "Epoch 60 | Batch 200 | Loss 1.46115\n",
      "Epoch 60 | Batch 300 | Loss 1.46115\n",
      "Epoch 60 | Batch 400 | Loss 1.46115\n",
      "Epoch 60 | Batch 500 | Loss 1.46116\n",
      "Epoch 60 | Batch 600 | Loss 1.47678\n",
      "Epoch 60 | Batch 700 | Loss 1.46115\n",
      "Epoch 60 | Training Loss 1.47122 | Validation Loss 1.47063\n",
      "Epoch 61 | Batch 100 | Loss 1.49236\n",
      "Epoch 61 | Batch 200 | Loss 1.48394\n",
      "Epoch 61 | Batch 300 | Loss 1.47678\n",
      "Epoch 61 | Batch 400 | Loss 1.47143\n",
      "Epoch 61 | Batch 500 | Loss 1.47678\n",
      "Epoch 61 | Batch 600 | Loss 1.46115\n",
      "Epoch 61 | Batch 700 | Loss 1.49266\n",
      "Epoch 61 | Training Loss 1.47084 | Validation Loss 1.46943\n",
      "Epoch 62 | Batch 100 | Loss 1.46117\n",
      "Epoch 62 | Batch 200 | Loss 1.47636\n",
      "Epoch 62 | Batch 300 | Loss 1.47678\n",
      "Epoch 62 | Batch 400 | Loss 1.46115\n",
      "Epoch 62 | Batch 500 | Loss 1.47678\n",
      "Epoch 62 | Batch 600 | Loss 1.47678\n",
      "Epoch 62 | Batch 700 | Loss 1.46115\n",
      "Epoch 62 | Training Loss 1.47009 | Validation Loss 1.47057\n",
      "Epoch 63 | Batch 100 | Loss 1.47677\n",
      "Epoch 63 | Batch 200 | Loss 1.47678\n",
      "Epoch 63 | Batch 300 | Loss 1.46115\n",
      "Epoch 63 | Batch 400 | Loss 1.47570\n",
      "Epoch 63 | Batch 500 | Loss 1.48198\n",
      "Epoch 63 | Batch 600 | Loss 1.49240\n",
      "Epoch 63 | Batch 700 | Loss 1.49016\n",
      "Epoch 63 | Training Loss 1.46943 | Validation Loss 1.47217\n",
      "Epoch 64 | Batch 100 | Loss 1.46115\n",
      "Epoch 64 | Batch 200 | Loss 1.46115\n",
      "Epoch 64 | Batch 300 | Loss 1.49284\n",
      "Epoch 64 | Batch 400 | Loss 1.46115\n",
      "Epoch 64 | Batch 500 | Loss 1.46178\n",
      "Epoch 64 | Batch 600 | Loss 1.49240\n",
      "Epoch 64 | Batch 700 | Loss 1.46115\n",
      "Epoch 64 | Training Loss 1.47047 | Validation Loss 1.47446\n",
      "Epoch 65 | Batch 100 | Loss 1.46115\n",
      "Epoch 65 | Batch 200 | Loss 1.47678\n",
      "Epoch 65 | Batch 300 | Loss 1.47678\n",
      "Epoch 65 | Batch 400 | Loss 1.46117\n",
      "Epoch 65 | Batch 500 | Loss 1.46115\n",
      "Epoch 65 | Batch 600 | Loss 1.47678\n",
      "Epoch 65 | Batch 700 | Loss 1.46115\n",
      "Epoch 65 | Training Loss 1.47031 | Validation Loss 1.46983\n",
      "Epoch 66 | Batch 100 | Loss 1.46115\n",
      "Epoch 66 | Batch 200 | Loss 1.46115\n",
      "Epoch 66 | Batch 300 | Loss 1.47561\n",
      "Epoch 66 | Batch 400 | Loss 1.47641\n",
      "Epoch 66 | Batch 500 | Loss 1.47678\n",
      "Epoch 66 | Batch 600 | Loss 1.47678\n",
      "Epoch 66 | Batch 700 | Loss 1.46115\n",
      "Epoch 66 | Training Loss 1.46887 | Validation Loss 1.47072\n",
      "Epoch 67 | Batch 100 | Loss 1.49239\n",
      "Epoch 67 | Batch 200 | Loss 1.47678\n",
      "Epoch 67 | Batch 300 | Loss 1.46115\n",
      "Epoch 67 | Batch 400 | Loss 1.49243\n",
      "Epoch 67 | Batch 500 | Loss 1.46374\n",
      "Epoch 67 | Batch 600 | Loss 1.46115\n",
      "Epoch 67 | Batch 700 | Loss 1.46115\n",
      "Epoch 67 | Training Loss 1.46995 | Validation Loss 1.47171\n",
      "Epoch 68 | Batch 100 | Loss 1.46115\n",
      "Epoch 68 | Batch 200 | Loss 1.46115\n",
      "Epoch 68 | Batch 300 | Loss 1.46115\n",
      "Epoch 68 | Batch 400 | Loss 1.47678\n",
      "Epoch 68 | Batch 500 | Loss 1.47181\n",
      "Epoch 68 | Batch 600 | Loss 1.46115\n",
      "Epoch 68 | Batch 700 | Loss 1.46115\n",
      "Epoch 68 | Training Loss 1.47011 | Validation Loss 1.47017\n",
      "Epoch 69 | Batch 100 | Loss 1.46115\n",
      "Epoch 69 | Batch 200 | Loss 1.47438\n",
      "Epoch 69 | Batch 300 | Loss 1.49144\n",
      "Epoch 69 | Batch 400 | Loss 1.47678\n",
      "Epoch 69 | Batch 500 | Loss 1.47678\n",
      "Epoch 69 | Batch 600 | Loss 1.46115\n",
      "Epoch 69 | Batch 700 | Loss 1.47676\n",
      "Epoch 69 | Training Loss 1.47140 | Validation Loss 1.46998\n",
      "Epoch 70 | Batch 100 | Loss 1.46115\n",
      "Epoch 70 | Batch 200 | Loss 1.46623\n",
      "Epoch 70 | Batch 300 | Loss 1.47678\n",
      "Epoch 70 | Batch 400 | Loss 1.46115\n",
      "Epoch 70 | Batch 500 | Loss 1.46115\n",
      "Epoch 70 | Batch 600 | Loss 1.46115\n",
      "Epoch 70 | Batch 700 | Loss 1.46115\n",
      "Epoch 70 | Training Loss 1.46938 | Validation Loss 1.47070\n",
      "Epoch 71 | Batch 100 | Loss 1.46115\n",
      "Epoch 71 | Batch 200 | Loss 1.46115\n",
      "Epoch 71 | Batch 300 | Loss 1.46115\n",
      "Epoch 71 | Batch 400 | Loss 1.46115\n",
      "Epoch 71 | Batch 500 | Loss 1.46139\n",
      "Epoch 71 | Batch 600 | Loss 1.46170\n",
      "Epoch 71 | Batch 700 | Loss 1.46115\n",
      "Epoch 71 | Training Loss 1.46862 | Validation Loss 1.47155\n",
      "Epoch 72 | Batch 100 | Loss 1.46119\n",
      "Epoch 72 | Batch 200 | Loss 1.46317\n",
      "Epoch 72 | Batch 300 | Loss 1.49242\n",
      "Epoch 72 | Batch 400 | Loss 1.47678\n",
      "Epoch 72 | Batch 500 | Loss 1.47678\n",
      "Epoch 72 | Batch 600 | Loss 1.46115\n",
      "Epoch 72 | Batch 700 | Loss 1.46115\n",
      "Epoch 72 | Training Loss 1.46944 | Validation Loss 1.47258\n",
      "Epoch 73 | Batch 100 | Loss 1.46115\n",
      "Epoch 73 | Batch 200 | Loss 1.46115\n",
      "Epoch 73 | Batch 300 | Loss 1.46115\n",
      "Epoch 73 | Batch 400 | Loss 1.47666\n",
      "Epoch 73 | Batch 500 | Loss 1.46115\n",
      "Epoch 73 | Batch 600 | Loss 1.46115\n",
      "Epoch 73 | Batch 700 | Loss 1.46115\n",
      "Epoch 73 | Training Loss 1.46912 | Validation Loss 1.47072\n",
      "Epoch 74 | Batch 100 | Loss 1.46115\n",
      "Epoch 74 | Batch 200 | Loss 1.47640\n",
      "Epoch 74 | Batch 300 | Loss 1.46115\n",
      "Epoch 74 | Batch 400 | Loss 1.46121\n",
      "Epoch 74 | Batch 500 | Loss 1.46115\n",
      "Epoch 74 | Batch 600 | Loss 1.46115\n",
      "Epoch 74 | Batch 700 | Loss 1.49240\n",
      "Epoch 74 | Training Loss 1.46992 | Validation Loss 1.47257\n",
      "Epoch 75 | Batch 100 | Loss 1.46115\n",
      "Epoch 75 | Batch 200 | Loss 1.47677\n",
      "Epoch 75 | Batch 300 | Loss 1.50983\n",
      "Epoch 75 | Batch 400 | Loss 1.46115\n",
      "Epoch 75 | Batch 500 | Loss 1.47678\n",
      "Epoch 75 | Batch 600 | Loss 1.47638\n",
      "Epoch 75 | Batch 700 | Loss 1.47678\n",
      "Epoch 75 | Training Loss 1.46978 | Validation Loss 1.47111\n",
      "Epoch 76 | Batch 100 | Loss 1.46115\n",
      "Epoch 76 | Batch 200 | Loss 1.46115\n",
      "Epoch 76 | Batch 300 | Loss 1.49240\n",
      "Epoch 76 | Batch 400 | Loss 1.46115\n",
      "Epoch 76 | Batch 500 | Loss 1.46735\n",
      "Epoch 76 | Batch 600 | Loss 1.46115\n",
      "Epoch 76 | Batch 700 | Loss 1.47678\n",
      "Epoch 76 | Training Loss 1.46898 | Validation Loss 1.46983\n",
      "Epoch 77 | Batch 100 | Loss 1.47678\n",
      "Epoch 77 | Batch 200 | Loss 1.46464\n",
      "Epoch 77 | Batch 300 | Loss 1.47678\n",
      "Epoch 77 | Batch 400 | Loss 1.47662\n",
      "Epoch 77 | Batch 500 | Loss 1.46115\n",
      "Epoch 77 | Batch 600 | Loss 1.46116\n",
      "Epoch 77 | Batch 700 | Loss 1.46115\n",
      "Epoch 77 | Training Loss 1.46935 | Validation Loss 1.47033\n",
      "Epoch 78 | Batch 100 | Loss 1.46115\n",
      "Epoch 78 | Batch 200 | Loss 1.48607\n",
      "Epoch 78 | Batch 300 | Loss 1.50803\n",
      "Epoch 78 | Batch 400 | Loss 1.47678\n",
      "Epoch 78 | Batch 500 | Loss 1.47678\n",
      "Epoch 78 | Batch 600 | Loss 1.46115\n",
      "Epoch 78 | Batch 700 | Loss 1.46115\n",
      "Epoch 78 | Training Loss 1.46944 | Validation Loss 1.46954\n",
      "Epoch 79 | Batch 100 | Loss 1.46115\n",
      "Epoch 79 | Batch 200 | Loss 1.46115\n",
      "Epoch 79 | Batch 300 | Loss 1.47678\n",
      "Epoch 79 | Batch 400 | Loss 1.48691\n",
      "Epoch 79 | Batch 500 | Loss 1.46115\n",
      "Epoch 79 | Batch 600 | Loss 1.46115\n",
      "Epoch 79 | Batch 700 | Loss 1.46115\n",
      "Epoch 79 | Training Loss 1.46865 | Validation Loss 1.46983\n",
      "Epoch 80 | Batch 100 | Loss 1.47679\n",
      "Epoch 80 | Batch 200 | Loss 1.47678\n",
      "Epoch 80 | Batch 300 | Loss 1.49240\n",
      "Epoch 80 | Batch 400 | Loss 1.47596\n",
      "Epoch 80 | Batch 500 | Loss 1.47678\n",
      "Epoch 80 | Batch 600 | Loss 1.46115\n",
      "Epoch 80 | Batch 700 | Loss 1.49240\n",
      "Epoch 80 | Training Loss 1.46899 | Validation Loss 1.47196\n",
      "Epoch 81 | Batch 100 | Loss 1.46115\n",
      "Epoch 81 | Batch 200 | Loss 1.46115\n",
      "Epoch 81 | Batch 300 | Loss 1.46115\n",
      "Epoch 81 | Batch 400 | Loss 1.46115\n",
      "Epoch 81 | Batch 500 | Loss 1.46776\n",
      "Epoch 81 | Batch 600 | Loss 1.46115\n",
      "Epoch 81 | Batch 700 | Loss 1.46115\n",
      "Epoch 81 | Training Loss 1.46940 | Validation Loss 1.46973\n",
      "Epoch 82 | Batch 100 | Loss 1.47670\n",
      "Epoch 82 | Batch 200 | Loss 1.46115\n",
      "Epoch 82 | Batch 300 | Loss 1.46115\n",
      "Epoch 82 | Batch 400 | Loss 1.46115\n",
      "Epoch 82 | Batch 500 | Loss 1.47677\n",
      "Epoch 82 | Batch 600 | Loss 1.49239\n",
      "Epoch 82 | Batch 700 | Loss 1.46115\n",
      "Epoch 82 | Training Loss 1.46953 | Validation Loss 1.47152\n",
      "Epoch 83 | Batch 100 | Loss 1.46115\n",
      "Epoch 83 | Batch 200 | Loss 1.49240\n",
      "Epoch 83 | Batch 300 | Loss 1.46115\n",
      "Epoch 83 | Batch 400 | Loss 1.46116\n",
      "Epoch 83 | Batch 500 | Loss 1.46115\n",
      "Epoch 83 | Batch 600 | Loss 1.46123\n",
      "Epoch 83 | Batch 700 | Loss 1.47677\n",
      "Epoch 83 | Training Loss 1.46905 | Validation Loss 1.46919\n",
      "Epoch 84 | Batch 100 | Loss 1.46396\n",
      "Epoch 84 | Batch 200 | Loss 1.46115\n",
      "Epoch 84 | Batch 300 | Loss 1.46115\n",
      "Epoch 84 | Batch 400 | Loss 1.50073\n",
      "Epoch 84 | Batch 500 | Loss 1.47678\n",
      "Epoch 84 | Batch 600 | Loss 1.49240\n",
      "Epoch 84 | Batch 700 | Loss 1.46115\n",
      "Epoch 84 | Training Loss 1.46851 | Validation Loss 1.47047\n",
      "Epoch 85 | Batch 100 | Loss 1.47346\n",
      "Epoch 85 | Batch 200 | Loss 1.46128\n",
      "Epoch 85 | Batch 300 | Loss 1.46118\n",
      "Epoch 85 | Batch 400 | Loss 1.46115\n",
      "Epoch 85 | Batch 500 | Loss 1.46115\n",
      "Epoch 85 | Batch 600 | Loss 1.50030\n",
      "Epoch 85 | Batch 700 | Loss 1.46129\n",
      "Epoch 85 | Training Loss 1.46769 | Validation Loss 1.46910\n",
      "Epoch 86 | Batch 100 | Loss 1.46115\n",
      "Epoch 86 | Batch 200 | Loss 1.46115\n",
      "Epoch 86 | Batch 300 | Loss 1.46115\n",
      "Epoch 86 | Batch 400 | Loss 1.46115\n",
      "Epoch 86 | Batch 500 | Loss 1.48922\n",
      "Epoch 86 | Batch 600 | Loss 1.46115\n",
      "Epoch 86 | Batch 700 | Loss 1.49240\n",
      "Epoch 86 | Training Loss 1.46838 | Validation Loss 1.47048\n",
      "Epoch 87 | Batch 100 | Loss 1.46115\n",
      "Epoch 87 | Batch 200 | Loss 1.47678\n",
      "Epoch 87 | Batch 300 | Loss 1.46738\n",
      "Epoch 87 | Batch 400 | Loss 1.46115\n",
      "Epoch 87 | Batch 500 | Loss 1.46115\n",
      "Epoch 87 | Batch 600 | Loss 1.46120\n",
      "Epoch 87 | Batch 700 | Loss 1.47632\n",
      "Epoch 87 | Training Loss 1.46862 | Validation Loss 1.46933\n",
      "Epoch 88 | Batch 100 | Loss 1.46115\n",
      "Epoch 88 | Batch 200 | Loss 1.46251\n",
      "Epoch 88 | Batch 300 | Loss 1.46115\n",
      "Epoch 88 | Batch 400 | Loss 1.46115\n",
      "Epoch 88 | Batch 500 | Loss 1.46934\n",
      "Epoch 88 | Batch 600 | Loss 1.46115\n",
      "Epoch 88 | Batch 700 | Loss 1.47678\n",
      "Epoch 88 | Training Loss 1.46805 | Validation Loss 1.46779\n",
      "Epoch 89 | Batch 100 | Loss 1.46115\n",
      "Epoch 89 | Batch 200 | Loss 1.46115\n",
      "Epoch 89 | Batch 300 | Loss 1.46115\n",
      "Epoch 89 | Batch 400 | Loss 1.47678\n",
      "Epoch 89 | Batch 500 | Loss 1.49186\n",
      "Epoch 89 | Batch 600 | Loss 1.46115\n",
      "Epoch 89 | Batch 700 | Loss 1.47678\n",
      "Epoch 89 | Training Loss 1.46828 | Validation Loss 1.46889\n",
      "Epoch 90 | Batch 100 | Loss 1.49057\n",
      "Epoch 90 | Batch 200 | Loss 1.46115\n",
      "Epoch 90 | Batch 300 | Loss 1.49228\n",
      "Epoch 90 | Batch 400 | Loss 1.46115\n",
      "Epoch 90 | Batch 500 | Loss 1.50689\n",
      "Epoch 90 | Batch 600 | Loss 1.46116\n",
      "Epoch 90 | Batch 700 | Loss 1.46115\n",
      "Epoch 90 | Training Loss 1.46798 | Validation Loss 1.46959\n",
      "Epoch 91 | Batch 100 | Loss 1.47212\n",
      "Epoch 91 | Batch 200 | Loss 1.47678\n",
      "Epoch 91 | Batch 300 | Loss 1.46116\n",
      "Epoch 91 | Batch 400 | Loss 1.46115\n",
      "Epoch 91 | Batch 500 | Loss 1.46115\n",
      "Epoch 91 | Batch 600 | Loss 1.46115\n",
      "Epoch 91 | Batch 700 | Loss 1.46115\n",
      "Epoch 91 | Training Loss 1.46723 | Validation Loss 1.46987\n",
      "Epoch 92 | Batch 100 | Loss 1.46119\n",
      "Epoch 92 | Batch 200 | Loss 1.46115\n",
      "Epoch 92 | Batch 300 | Loss 1.47676\n",
      "Epoch 92 | Batch 400 | Loss 1.47653\n",
      "Epoch 92 | Batch 500 | Loss 1.47684\n",
      "Epoch 92 | Batch 600 | Loss 1.46119\n",
      "Epoch 92 | Batch 700 | Loss 1.46115\n",
      "Epoch 92 | Training Loss 1.46828 | Validation Loss 1.47011\n",
      "Epoch 93 | Batch 100 | Loss 1.46115\n",
      "Epoch 93 | Batch 200 | Loss 1.47677\n",
      "Epoch 93 | Batch 300 | Loss 1.46115\n",
      "Epoch 93 | Batch 400 | Loss 1.49154\n",
      "Epoch 93 | Batch 500 | Loss 1.49240\n",
      "Epoch 93 | Batch 600 | Loss 1.46115\n",
      "Epoch 93 | Batch 700 | Loss 1.46115\n",
      "Epoch 93 | Training Loss 1.46720 | Validation Loss 1.46870\n",
      "Epoch 94 | Batch 100 | Loss 1.46152\n",
      "Epoch 94 | Batch 200 | Loss 1.46115\n",
      "Epoch 94 | Batch 300 | Loss 1.47678\n",
      "Epoch 94 | Batch 400 | Loss 1.47678\n",
      "Epoch 94 | Batch 500 | Loss 1.46116\n",
      "Epoch 94 | Batch 600 | Loss 1.46115\n",
      "Epoch 94 | Batch 700 | Loss 1.46115\n",
      "Epoch 94 | Training Loss 1.46765 | Validation Loss 1.46800\n",
      "Epoch 95 | Batch 100 | Loss 1.47678\n",
      "Epoch 95 | Batch 200 | Loss 1.46115\n",
      "Epoch 95 | Batch 300 | Loss 1.47673\n",
      "Epoch 95 | Batch 400 | Loss 1.47678\n",
      "Epoch 95 | Batch 500 | Loss 1.46115\n",
      "Epoch 95 | Batch 600 | Loss 1.46115\n",
      "Epoch 95 | Batch 700 | Loss 1.47677\n",
      "Epoch 95 | Training Loss 1.46756 | Validation Loss 1.46969\n",
      "Epoch 96 | Batch 100 | Loss 1.47678\n",
      "Epoch 96 | Batch 200 | Loss 1.46115\n",
      "Epoch 96 | Batch 300 | Loss 1.46115\n",
      "Epoch 96 | Batch 400 | Loss 1.47678\n",
      "Epoch 96 | Batch 500 | Loss 1.46115\n",
      "Epoch 96 | Batch 600 | Loss 1.47678\n",
      "Epoch 96 | Batch 700 | Loss 1.46115\n",
      "Epoch 96 | Training Loss 1.46883 | Validation Loss 1.47079\n",
      "Epoch 97 | Batch 100 | Loss 1.46115\n",
      "Epoch 97 | Batch 200 | Loss 1.46115\n",
      "Epoch 97 | Batch 300 | Loss 1.46115\n",
      "Epoch 97 | Batch 400 | Loss 1.46115\n",
      "Epoch 97 | Batch 500 | Loss 1.46116\n",
      "Epoch 97 | Batch 600 | Loss 1.46115\n",
      "Epoch 97 | Batch 700 | Loss 1.47888\n",
      "Epoch 97 | Training Loss 1.46832 | Validation Loss 1.46983\n",
      "Epoch 98 | Batch 100 | Loss 1.46224\n",
      "Epoch 98 | Batch 200 | Loss 1.46115\n",
      "Epoch 98 | Batch 300 | Loss 1.46117\n",
      "Epoch 98 | Batch 400 | Loss 1.46192\n",
      "Epoch 98 | Batch 500 | Loss 1.46115\n",
      "Epoch 98 | Batch 600 | Loss 1.47677\n",
      "Epoch 98 | Batch 700 | Loss 1.46323\n",
      "Epoch 98 | Training Loss 1.46749 | Validation Loss 1.46959\n",
      "Epoch 99 | Batch 100 | Loss 1.46115\n",
      "Epoch 99 | Batch 200 | Loss 1.46115\n",
      "Epoch 99 | Batch 300 | Loss 1.47678\n",
      "Epoch 99 | Batch 400 | Loss 1.47690\n",
      "Epoch 99 | Batch 500 | Loss 1.46115\n",
      "Epoch 99 | Batch 600 | Loss 1.46115\n",
      "Epoch 99 | Batch 700 | Loss 1.47613\n",
      "Epoch 99 | Training Loss 1.46770 | Validation Loss 1.47073\n"
     ]
    }
   ],
   "source": [
    "# Updated training loop with validation loss after every epoch\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_train_loss = 0\n",
    "    for batch_idx, (train_data, train_labels) in enumerate(train_loader):\n",
    "        # put data on device\n",
    "        train_data = train_data.to(DEVICE)\n",
    "        train_labels = train_labels.to(DEVICE)\n",
    "\n",
    "        # run forward pass\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(train_data)\n",
    "\n",
    "        # calculate loss and backwards pass\n",
    "        loss = F.cross_entropy(pred, train_labels)\n",
    "        # train_loss.append(loss.item())\n",
    "        running_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # every 100 batches output loss\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx} | Loss {loss:0.5f}\")\n",
    "\n",
    "    avg_train_loss = running_train_loss / (batch_idx + 1)\n",
    "    \n",
    "    train_loss.append(avg_train_loss)\n",
    "    # after every epoch track the validation loss\n",
    "    model.eval()\n",
    "\n",
    "    running_val_loss = 0\n",
    "\n",
    "    for batch_idx, (val_data, val_labels) in enumerate(val_loader):\n",
    "        # put data on device\n",
    "        val_data = val_data.to(DEVICE)\n",
    "        val_labels = val_labels.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(val_data)\n",
    "            loss = F.cross_entropy(pred, val_labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / (batch_idx + 1)\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Training Loss {avg_train_loss:0.5f} | Validation Loss {avg_val_loss:0.5f}\")\n",
    "\n",
    "    val_loss.append(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_loss, label=\"train loss\")\n",
    "plt.plot(val_loss, label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.46989\n",
      "test accuracy: 0.98646\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (example_data, example_targets) in enumerate(test_loader):\n",
    "            example_data = example_data.to(DEVICE)\n",
    "            example_targets = example_targets.to(DEVICE)\n",
    "            # run forward pass\n",
    "            output = model(example_data)\n",
    "            test_loss += F.cross_entropy(output, example_targets)\n",
    "            output = torch.argmax(output, dim=1)\n",
    "            correct += sum(output == example_targets)\n",
    "\n",
    "print(f'test loss: {test_loss/len(test_loader):0.5f}')\n",
    "print(f'test accuracy: {correct/len(test_loader)/BATCH_SIZE:0.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'MNIST_CNN.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress tracker:\n",
    "- 1 layer NN with 10 epochs of training: \n",
    "    - test loss: 1.61052\n",
    "    - test accuracy: 0.88704\n",
    "    \n",
    "- 2 layer NN with 64 size hidden layer, added AdamW optimizer:\n",
    "    - test loss: 1.53824\n",
    "    - test accuracy: 0.92038\n",
    "\n",
    "- 2 layer NN with 128 size hidden layer: (practically no improvement)\n",
    "    - test loss: 1.53825\n",
    "    - test accuracy: 0.91929\n",
    "\n",
    "- 2 layer NN with 64 size hidden layer, added relu after first layer: (relu was necessary ow just liek 1 layer NN)\n",
    "    - test loss: 1.49081\n",
    "    - test accuracy: 0.96865\n",
    "\n",
    "- 3 layer NN with 128 size hidden layer: (architecture is limited)\n",
    "    - test loss: 1.48965\n",
    "    - test accuracy: 0.96785\n",
    "\n",
    "- 1 layer CNN with kernel 3 and padding = 1 joined iwth 1 layer Linear layer: (( appears that regresses to 2 layer NN bc no abstraciton of data))\n",
    "    - test loss: 1.53574\n",
    "    - test accuracy: 0.92267\n",
    "\n",
    "- 2 layer CNN with kernel 3, no padding and increasing channel by 2:\n",
    "    - test loss: 1.49036\n",
    "    - test accuracy: 0.96676\n",
    "\n",
    "- added maxpooling with kernel size 2 in-between conv layers:\n",
    "    - test loss: 1.49294\n",
    "    - test accuracy: 0.96537\n",
    "\n",
    "- relu inbetween convlution layers and deeper channels to 4 and 16: otherwise siimlar to single convlution layer:\n",
    "    - test loss: 1.48902\n",
    "    - test accuracy: 0.97014\n",
    "\n",
    "- larger kernel size - 5:\n",
    "    - test loss: 1.47957\n",
    "    - test accuracy: 0.97850\n",
    "\n",
    "- second linear layer:\n",
    "    - test loss: 1.47536\n",
    "    - test accuracy: 0.98278\n",
    "\n",
    "- dropout for overfitting, only after dense layers: (possibly overfitting after running 10 epochs)\n",
    "    - test loss: 1.47994\n",
    "    - test accuracy: 0.97751\n",
    "\n",
    "- dropout in between convolution layers 20 epoch before overfitting:\n",
    "    - test loss: 1.47057\n",
    "    - test accuracy: 0.98597"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
